{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import joblib\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the backend_access_data.csv file\n",
    "backend_data = pd.read_csv('backend_access_data.csv')\n",
    "\n",
    "# Read the frontend_access_data.csv file\n",
    "frontend_data = pd.read_csv('frontend_access_data.csv')\n",
    "\n",
    "# Read the metrics.csv file\n",
    "metrics_data = pd.read_csv('metrics.csv')\n",
    "\n",
    "# Merge the backend and frontend data on the \"Request ID\" column\n",
    "merged_data = pd.merge(backend_data, frontend_data, on='Request ID', suffixes=('_backend', '_frontend'))\n",
    "\n",
    "merged_data = pd.merge(merged_data, metrics_data, on='Endpoint', suffixes=('_request', '_metric'))\n",
    "\n",
    "# Group the rows by endpoints and calculate the average request time of each endpoint\n",
    "grouped_data = merged_data.groupby('Endpoint')['Request Time'].mean()\n",
    "\n",
    "# Add a new feature \"average request time\", and give it the value obtained earlier\n",
    "merged_data['Average Request Time'] = merged_data['Endpoint'].map(grouped_data)\n",
    "\n",
    "# Initialize a Robust Scaler\n",
    "scaler = RobustScaler()\n",
    "\n",
    "# Fit the scaler to the 'Request Time' column and transform it\n",
    "merged_data['Request Time'] = scaler.fit_transform(merged_data[['Request Time']])\n",
    "\n",
    "# Split data into two datasets: one with isCached as true and another as false\n",
    "cached_data = merged_data[merged_data['isCached'] == True]\n",
    "not_cached_data = merged_data[merged_data['isCached'] == False]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_selection(target_attribute, data):\n",
    "    # Split the data into input and output\n",
    "    X = data.drop(['CPU Usage', 'Memory Usage', 'Storage Usage', 'Request ID', 'Query', 'Endpoint'], axis=1)\n",
    "    Y = data[target_attribute]\n",
    "\n",
    "    categorical_columns = ['HTTP method', 'Pricing Plan', 'Response Status', 'Cache Used']\n",
    "    X = pd.get_dummies(X, columns=categorical_columns)\n",
    "\n",
    "    # Create a GBM model\n",
    "    model = GradientBoostingRegressor()\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(X, Y)\n",
    "\n",
    "    # Create a selector object that will use the GBM model to identify\n",
    "    # features that have an importance of more than 0.15\n",
    "    sfm = SelectFromModel(model, threshold=0.15)\n",
    "\n",
    "    # Train the selector\n",
    "    sfm.fit(X, Y)\n",
    "\n",
    "    return X.columns[sfm.get_support(indices=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Usage: Index(['Request Time', 'Concurrent requests'], dtype='object')\n",
      "Memory Usage: Index(['Request Size', 'Concurrent requests'], dtype='object')\n",
      "Storage Usage: Index(['Request Size', 'Concurrent requests'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "cpu_usage_feature_selection_cache = feature_selection('CPU Usage', cached_data)\n",
    "cpu_usage_feature_selection_not_cache = feature_selection('CPU Usage', not_cached_data)\n",
    "memory_usage_feature_selection_cache = feature_selection('Memory Usage', cached_data)\n",
    "memory_usage_feature_selection_not_cache = feature_selection('Memory Usage', not_cached_data)\n",
    "storage_usage_feature_selection_cache = feature_selection('Storage Usage', cached_data)\n",
    "storage_usage_feature_selection_not_cache = feature_selection('Storage Usage', not_cached_data)\n",
    "\n",
    "print('Cached CPU Usage: ' + str(cpu_usage_feature_selection_cache))\n",
    "print('Not cached CPU Usage: ' + str(cpu_usage_feature_selection_not_cache))\n",
    "print('Cached Memory Usage: '+ str(memory_usage_feature_selection_cache))\n",
    "print('Not cached Memory Usage: '+ str(memory_usage_feature_selection_not_cache))\n",
    "print('Cached Storage Usage: '+ str(storage_usage_feature_selection_cache))\n",
    "print('Not cached Storage Usage: '+ str(storage_usage_feature_selection_not_cache))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(target_attribute, selected_feature_names, data, model=GradientBoostingRegressor()):\n",
    "    # Select the features from the data\n",
    "    X = data\n",
    "    Y = data[target_attribute]\n",
    "\n",
    "    categorical_columns = ['HTTP method', 'Pricing Plan', 'Response Status', 'Cache Used']\n",
    "    X = pd.get_dummies(X, columns=categorical_columns)\n",
    "    \n",
    "    X = X[selected_feature_names]\n",
    "\n",
    "    # Split the data into training and test sets\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(X_train, Y_train)\n",
    "\n",
    "    evaluation = evaluate_model(model, X_test, Y_test)\n",
    "\n",
    "    return model, evaluation\n",
    "\n",
    "def evaluate_model(model, X_test, Y_test):\n",
    "    # Use the model to make predictions on the test set\n",
    "    predictions = model.predict(X_test)\n",
    "\n",
    "    # Compute evaluation metrics\n",
    "    mae = mean_absolute_error(Y_test, predictions)\n",
    "    mse = mean_squared_error(Y_test, predictions)\n",
    "    rmse = sqrt(mse)\n",
    "    r2 = r2_score(Y_test, predictions)\n",
    "\n",
    "    return [mae, mse, rmse, r2]\n",
    "\n",
    "def predict(model, new_data):\n",
    "    # Use the trained model to make predictions on the new data\n",
    "    predictions = model.predict(new_data)\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Usage: [9.845332029934614, 150.4079031132224, 12.264089983085675, 0.5384065910209282]\n",
      "Memory Usage: [967.6776716722032, 1515675.484895769, 1231.1277289118984, 0.372435567330599]\n",
      "Storage Usage: [979.8664161117212, 1517647.3518294564, 1231.92830628631, 0.5417998782016276]\n"
     ]
    }
   ],
   "source": [
    "model = GradientBoostingRegressor()\n",
    "\n",
    "cpu_usage_model_cache, cpu_usage_evaluation_cache = train_model('CPU Usage', cpu_usage_feature_selection_cache, cached_data, model)\n",
    "memory_usage_model_cache, memory_usage_evaluation_cache = train_model('Memory Usage', memory_usage_feature_selection_cache, cached_data, model)\n",
    "storage_usage_model_cache, storage_usage_evaluation_cache = train_model('Storage Usage', storage_usage_feature_selection_cache, cached_data, model)\n",
    "\n",
    "cpu_usage_model_not_cache, cpu_usage_evaluation_not_cache = train_model('CPU Usage', cpu_usage_feature_selection_not_cache, not_cached_data, model)\n",
    "memory_usage_model_not_cache, memory_usage_evaluation_not_cache = train_model('Memory Usage', memory_usage_feature_selection_not_cache, not_cached_data, model)\n",
    "storage_usage_model_not_cache, storage_usage_evaluation_not_cache = train_model('Storage Usage', storage_usage_feature_selection_not_cache, not_cached_data, model)\n",
    "\n",
    "print('Cached CPU Usage: ' + str(cpu_usage_evaluation_cache))\n",
    "print('Not cached CPU Usage: ' + str(cpu_usage_evaluation_not_cache))\n",
    "print('Cached Memory Usage: '+ str(memory_usage_evaluation_cache))\n",
    "print('Not cached Memory Usage: '+ str(memory_usage_evaluation_not_cache))\n",
    "print('Cached Storage Usage: '+ str(storage_usage_evaluation_cache))\n",
    "print('Not cached Storage Usage: '+ str(storage_usage_evaluation_not_cache))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['storage_usage_model.pkl']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the models to disk\n",
    "joblib.dump(cpu_usage_model_cache, 'cpu_usage_model_cache.pkl')\n",
    "joblib.dump(cpu_usage_model_not_cache, 'cpu_usage_model_not_cache.pkl')\n",
    "joblib.dump(memory_usage_model_cache, 'memory_usage_model_cache.pkl')\n",
    "joblib.dump(memory_usage_model_not_cache, 'memory_usage_model_not_cache.pkl')\n",
    "joblib.dump(storage_usage_model_cache, 'storage_usage_model_cache.pkl')\n",
    "joblib.dump(storage_usage_model_not_cache, 'storage_usage_model_not_cache.pkl')\n",
    "\n",
    "# Save the selected features to disk\n",
    "joblib.dump(cpu_usage_feature_selection_cache, 'cpu_usage_features_cache.pkl')\n",
    "joblib.dump(cpu_usage_feature_selection_not_cache, 'cpu_usage_features_not_cache.pkl')\n",
    "joblib.dump(memory_usage_feature_selection_cache, 'memory_usage_features_cache.pkl')\n",
    "joblib.dump(memory_usage_feature_selection_not_cache, 'memory_usage_features_not_cache.pkl')\n",
    "joblib.dump(storage_usage_feature_selection_cache, 'storage_usage_features_cache.pkl')\n",
    "joblib.dump(storage_usage_feature_selection_not_cache, 'storage_usage_features_not_cache.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
